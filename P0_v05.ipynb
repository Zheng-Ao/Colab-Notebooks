{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P0_v05.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fKdAJ8l5F_yg",
        "X_w0ItLna28l",
        "eFTDdD4jgSP3",
        "wmFzeBC6qaHX",
        "m0KrE7E6LDCr",
        "VNIQr9XuakxG"
      ],
      "mount_file_id": "1Sqr4KjJKKhLBjnKrl_PrfHPUOMsYKDXX",
      "authorship_tag": "ABX9TyMFEfXSJWiEOPiq7UzH89md",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zheng-Ao/Colab-Notebooks/blob/main/P0_v05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 全局设置"
      ],
      "metadata": {
        "id": "fKdAJ8l5F_yg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CR_-xEl5Emv2"
      },
      "outputs": [],
      "source": [
        "'''常用库导入'''\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "raw_data_path = \"drive/MyDrive/P0/T10K.csv\"\n",
        "# 测试谷歌云端硬盘是否成功加载:\n",
        "# pd.read_csv(raw_data_path)\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "import torch\n",
        "print(torch.__version__)                                                        # 用于确定PyG的安装\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, Subset, ConcatDataset, DataLoader\n",
        "\n",
        "# 检查GPU:\n",
        "# !nvidia-smi\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "'''特殊库安装(魔法函数要防止Cell的最开始)'''\n",
        "# 我不想输出安装信息，虽然%%capture不是用来干这个的，但它能达到我想要的效果\n",
        "!pip install transformers\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "id": "q_Zxt-SIFo9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTILS.py"
      ],
      "metadata": {
        "id": "F_Rn1K8pE3Z0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Txt Embedding"
      ],
      "metadata": {
        "id": "X_w0ItLna28l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''构建txt_vecs，作为Dataset的第二个数据来源'''\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")      \n",
        "nlp_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
        "'''加载BERT这种大模型很耗时，因此在整个流程中应当让上面两行代码只执行一次。'''"
      ],
      "metadata": {
        "id": "Fc3zwgx_WRXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(raw_data_path)\n",
        "\n",
        "for i in tqdm(range(10000)):\n",
        "    ttl = df[\"patent_title\"].values[i]\n",
        "    inputs = tokenizer(ttl, return_tensors=\"pt\").to(device)\n",
        "    outputs = nlp_model(**inputs)\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "    cls_vec = last_hidden_states[:,0,:].clone().detach()\n",
        "    if i == 0:\n",
        "        txt_vecs = cls_vec\n",
        "    else:\n",
        "        txt_vecs = torch.cat([txt_vecs, cls_vec],dim=0)\n",
        "\n",
        "print(txt_vecs.shape)\n",
        "# should be [num_samples, embedding_dim] = [10000, 768]"
      ],
      "metadata": {
        "id": "TjwGItGlE5Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Mat = txt_vecs.cpu().numpy()\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# txt_pca = PCA(n_components=0.9, svd_solver = 'full')\n",
        "txt_pca = PCA(n_components = 5)"
      ],
      "metadata": {
        "id": "E_SqxF1chNgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 查看PCA的效果:\n",
        "txt_pca.fit(Mat)\n",
        "var = txt_pca.explained_variance_ratio_\n",
        "len(var), var.sum()"
      ],
      "metadata": {
        "id": "CaoLuPwphNVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Txt_Embedding = txt_pca.fit_transform(Mat)\n",
        "print(Txt_Embedding.shape)\n",
        "# should be [10000, 5] here"
      ],
      "metadata": {
        "id": "Wp1-4ZfciG91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## utils"
      ],
      "metadata": {
        "id": "eFTDdD4jgSP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "# target transform, 0:neg, 1:pos\n",
        "def LabelCorePa(ref, isd):\n",
        "    now = datetime.strptime(\"2022-01-01\", \"%Y-%m-%d\").year\n",
        "    years = now - datetime.strptime(isd, \"%Y-%m-%d\").year\n",
        "    score = ref/years       \n",
        "    label = int((score>0.5))\n",
        "    return label\n",
        "\n",
        "# Dataset\n",
        "class PatDataset(Dataset):\n",
        "    def __init__(self, raw_data_path, txt_vecs, transform = None, target_transform = LabelCorePa):\n",
        "        self.raw_data = pd.read_csv(raw_data_path)\n",
        "        self.txt_vecs = txt_vecs\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.raw_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Y          \n",
        "        ref = self.raw_data.at[idx, \"patent_num_cited_by_us_patents\"]\n",
        "        isd = self.raw_data.at[idx, \"patent_date\"]\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(ref, isd)\n",
        "            label = torch.tensor(label,dtype=torch.long)\n",
        "\n",
        "        # X\n",
        "        # INDEXs\n",
        "        num_claims = self.raw_data.at[idx, \"patent_num_claims\"]\n",
        "        num_b_cits = self.raw_data.at[idx, \"patent_num_us_patent_citations\"]\n",
        "        inventors = self.raw_data.at[idx, \"inventors\"]\n",
        "        num_inventors = len(eval(inventors))\n",
        "        assignees = self.raw_data.at[idx, \"assignees\"]\n",
        "        if assignees == \"[{'assignee_sequence': None, 'assignee_key_id': None}]\":\n",
        "            num_assignees = 0\n",
        "        else:\n",
        "            num_assignees = len(eval(assignees))\n",
        "        IPCs = self.raw_data.at[idx, \"IPCs\"]\n",
        "        num_ipcs = len(eval(IPCs))\n",
        "        indexs = torch.tensor([num_claims, num_b_cits, num_inventors, num_assignees, num_ipcs], dtype=torch.float32).to(device)\n",
        "        # TXT\n",
        "        txt_vec = torch.tensor(self.txt_vecs[idx], dtype = torch.float32)\n",
        "        txt_vec = txt_vec.to(device)\n",
        "\n",
        "        patent = torch.cat([indexs, txt_vec])\n",
        "        \n",
        "        return patent, label\n",
        "\n",
        "# Models\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            # nn.BatchNorm1d(input_size),\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, output_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_size, hidden_size)\n",
        "        self.conv2 = GCNConv(hidden_size, 2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        logits = self.conv2(x, edge_index)                   # (num_nodes, 2)\n",
        "        return logits                  # (num_nodes, 2)\n",
        "\n",
        "\n",
        "\n",
        "# Train\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        # batch: 第几个batch；X: 包含batch_size个feature vec.\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        output = model(X).to(device)\n",
        "        loss = loss_fn(output, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 20 == 0:                                                     # train_size/batch_size = 100, 每20个batch输出一次结果，共输出5次。\n",
        "            loss, current = loss.item(), batch * len(X)                         \n",
        "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "# Test\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            output = model(X)\n",
        "            # test_loss += loss_fn(output, y).item()\n",
        "            # correct += (output.argmax(1) == y).type(torch.float).sum().item()\n",
        "            \n",
        "            pred = output.argmax(1).cpu()\n",
        "            y = y.cpu()\n",
        "            if batch == 0:\n",
        "                Pred = pred\n",
        "                Y = y\n",
        "            else:\n",
        "                Pred = torch.cat((Pred, pred), dim = 0)\n",
        "                Y = torch.cat((Y, y), dim=0)\n",
        "\n",
        "    C_Mat = metrics.confusion_matrix(Y, Pred)\n",
        "    accuracy = metrics.accuracy_score(Y,Pred)\n",
        "    f1 = metrics.f1_score(Y, Pred)\n",
        "    recall = metrics.recall_score(Y, Pred)\n",
        "    precision = metrics.precision_score(Y, Pred)\n",
        "    print(C_Mat)\n",
        "    print(f\"acc:{accuracy:.4f}, f1:{f1:.4f}, recall:{recall:.4f}, prec:{precision:.4f}\")\n",
        "\n",
        "    # test_loss /= num_batches\n",
        "    # correct /= size\n",
        "    # print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "K8tSi8TZFQyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# config.py"
      ],
      "metadata": {
        "id": "wmFzeBC6qaHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HyperParams----Config.py\n",
        "hidden_size = 16\n",
        "output_size = 2\n",
        "\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 5e-4\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "num_epochs_to_print = epochs/10                                                 # 每隔10次输出一次Metrics\n",
        "\n",
        "num_train = 8000\n",
        "num_test = 2000"
      ],
      "metadata": {
        "id": "jKlYsn2aqdjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "DgQRy866G5JW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Net"
      ],
      "metadata": {
        "id": "m0KrE7E6LDCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PIPLINE\n",
        "# DATA TO FIT A MODEL\n",
        "dataset = PatDataset(raw_data_path=raw_data_path, txt_vecs = Txt_Embedding)\n",
        "training_indices = [i for i in range(num_train)]\n",
        "test_indices = [i for i in range(num_train,num_train+num_test)]\n",
        "training_data_all = Subset(dataset, training_indices)\n",
        "test_data = Subset(dataset, test_indices)\n",
        "\n",
        "# Make pos:neg in training set 1:1\n",
        "pos_indices = []\n",
        "neg_indices = []\n",
        "for i in range(num_train):\n",
        "    if training_data_all[i][1] == 1:\n",
        "        pos_indices.append(i)\n",
        "    else:\n",
        "        neg_indices.append(i)\n",
        "num_pos = len(pos_indices)\n",
        "num_neg = len(neg_indices)\n",
        "print(f\"NEG:POS = {num_neg}:{num_pos} = {num_neg/num_pos:.2f}\")\n",
        "neg_indices_sample = np.random.choice(neg_indices, num_pos, replace = False)\t \n",
        "training_data_pos = Subset(training_data_all, pos_indices)\n",
        "training_data_neg = Subset(training_data_all, neg_indices_sample)\n",
        "training_data = ConcatDataset([training_data_pos, training_data_neg])\n",
        "print(\"Training sample number:\",len(training_data))\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")"
      ],
      "metadata": {
        "id": "Cnjn-MyeOvYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIT A MODEL\n",
        "# PyTorch的逻辑是先初始化（喂超参），再进行函数计算（喂输入）\n",
        "input_size = len(dataset[0][0])\n",
        "model = SimpleNet(input_size, hidden_size, output_size).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "for t in range(epochs):\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    if t==0 or (t+1)%num_epochs_to_print == 0:\n",
        "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "        test_loop(train_dataloader, model, loss_fn)                                 # performance on training data\n",
        "        test_loop(test_dataloader, model, loss_fn)                                  # performance on test data\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "iZiFY_jDIhNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyG GNN Net"
      ],
      "metadata": {
        "id": "HZwU5sBFyly0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# edge_index, edge_attr\n",
        "df = pd.read_csv(raw_data_path)\n",
        "\n",
        "adj_ls = []\n",
        "edges = []\n",
        "\n",
        "for i in tqdm(range(10000)):\n",
        "    id = df.values[i][0]\n",
        "    b_cits = eval(df[\"cited_patents\"].values[i])\n",
        "    for b_cit in b_cits:\n",
        "        # 是否在专利数据库中\n",
        "        if b_cit[\"cited_patent_date\"]:\n",
        "            # 是否在本数据集中\n",
        "            if b_cit[\"cited_patent_number\"] in df[\"patent_number\"].values:\n",
        "                # print(i)\n",
        "                cit_id = df[df[\"patent_number\"].values == b_cit[\"cited_patent_number\"]].values[0][0]\n",
        "                adj_ls.append([id, cit_id])\n",
        "                date = df[\"patent_date\"].values[id]\n",
        "                cit_date = df[\"patent_date\"].values[cit_id]\n",
        "                date1 = datetime.strptime(date, \"%Y-%m-%d\")\n",
        "                date2 = datetime.strptime(cit_date, \"%Y-%m-%d\")\n",
        "                dist = date1-date2\n",
        "                dist = 365/dist.days\n",
        "                edges.append(dist)"
      ],
      "metadata": {
        "id": "-myLjEp_2AHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df[df[\"patent_number\"].values == '3930276'].values[0][0]\n",
        "# \"3930276\" in df[\"patent_number\"].values\n",
        "# edges\n",
        "ls = [i for i in range(10)]\n",
        "\n",
        "ls_1 = np.random.choice(ls,5, replace = False)\n",
        "ls_1"
      ],
      "metadata": {
        "id": "Xxur-qrM2SnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data.x, data.y\n",
        "for i in range(10000):\n",
        "    if i == 0:\n",
        "        x = dataset[i][0].unsqueeze(0)\n",
        "        y = dataset[i][1].unsqueeze(0)\n",
        "    else:\n",
        "        x = torch.cat([x, dataset[i][0].unsqueeze(0)])\n",
        "        y = torch.cat([y, dataset[i][1].unsqueeze(0)])\n",
        "\n",
        "# "
      ],
      "metadata": {
        "id": "JT6lZqVrIsnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape, x.dtype, y.shape, y.dtype"
      ],
      "metadata": {
        "id": "CtjHVuhNP3vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(pos_indices),len(neg_indices_sample))\n",
        "print(neg_indices_sample)\n",
        "# len = 10\n",
        "# indices = [1,2,4,5]\n",
        " \n",
        "# ls1 = [False]*10\n",
        "# t = torch.tensor(ls1)\n",
        "# t[indices] = True\n",
        "# t\n",
        "\n",
        "# for i in range(len(pos_indices)):\n",
        "#     if pos_indices[i] == neg_indices_sample[i]:\n",
        "#         print(i)\n",
        "\n",
        "ls = [False]*10000\n",
        "\n",
        "# ls[pos_indices] = True\n",
        "\n",
        "ls_t = torch.tensor(ls, dtype = bool)\n",
        "# ls_t[pos_indices] = True\n",
        "ls_t[neg_indices_sample] = True\n",
        "ls_t.sum()\n"
      ],
      "metadata": {
        "id": "7gcVPUeMMurI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data\n",
        "\n",
        "# num_nodes, num_node_features\n",
        "x = torch.tensor(x, dtype=torch.float32)\n",
        "\n",
        "# num_nodes, 1\n",
        "y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# 2, num_edges\n",
        "edge_index = torch.tensor(adj_ls, dtype=torch.long)\n",
        "\n",
        "# num_edges, num_edge_features\n",
        "edge_attr = torch.tensor(edges, dtype=torch.float32)\n",
        "\n",
        "# train-test split\n",
        "train_ls = [False]*num_train + [False]*num_test\n",
        "train_mask = torch.tensor(train_ls, dtype=bool)\n",
        "train_mask[pos_indices] = True\n",
        "train_mask[neg_indices_sample] = True\n",
        "test_ls = [False]*num_train + [True]*num_test\n",
        "test_mask = torch.tensor(test_ls, dtype=bool)\n",
        "\n",
        "\n",
        "data = Data(x=x, y=y, edge_index = edge_index.t().contiguous(), edge_attr=edge_attr, train_mask = train_mask, test_mask=test_mask)\n",
        "data = data.to(device)"
      ],
      "metadata": {
        "id": "fXfkKRT6CGHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of nodes: {data.num_nodes}') # 节点数量\n",
        "print(f'Number of edges: {data.num_edges}') # 边数量\n",
        "print(f'Number of node features: {data.num_node_features}') # 节点属性的维度\n",
        "print(f'Number of node features: {data.num_features}') # 同样是节点属性的维度\n",
        "print(f'Number of edge features: {data.num_edge_features}') # 边属性的维度\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}') # 平均节点度\n",
        "print(f'if edge indices are ordered and do not contain duplicate entries.: {data.is_coalesced()}') # 是否边是有序的同时不含有重复的边\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}') # 用作训练集的节点\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}') # 用作训练集的节点的数量\n",
        "print(f'Contains isolated nodes: {data.has_isolated_nodes()}') # 此图是否包含孤立的节点\n",
        "print(f'Contains self-loops: {data.has_self_loops()}')  # 此图是否包含自环的边\n",
        "print(f'Is undirected: {data.is_undirected()}')  # 此图是否是无向图"
      ],
      "metadata": {
        "id": "tfxaz5LtCGFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = len(dataset[0][0])\n",
        "\n",
        "model = GNN(input_size, hidden_size, output_size).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "model.train()\n",
        "for epoch in range(200):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "cCx5_Hl5CGCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "pred = model(data).argmax(dim=1)\n",
        "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
        "acc = int(correct) / int(data.test_mask.sum())\n",
        "print(f'Accuracy: {acc:.4f}')\n",
        "Pred = pred[data.test_mask].cpu()\n",
        "Y = data.y[data.test_mask].cpu()\n",
        "C_Mat = metrics.confusion_matrix(Y, Pred)\n",
        "accuracy = metrics.accuracy_score(Y,Pred)\n",
        "f1 = metrics.f1_score(Y, Pred)\n",
        "recall = metrics.recall_score(Y, Pred)\n",
        "precision = metrics.precision_score(Y, Pred)\n",
        "print(C_Mat)\n",
        "print(f\"acc:{accuracy:.4f}, f1:{f1:.4f}, recall:{recall:.4f}, prec:{precision:.4f}\")\n"
      ],
      "metadata": {
        "id": "D44a9N5TCF6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save&Load Model"
      ],
      "metadata": {
        "id": "VNIQr9XuakxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 保存模型权重至当前文件夹\n",
        "torch.save(model.state_dict(), 'model_weights.pth')                             "
      ],
      "metadata": {
        "id": "tTO_IOiiarkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleNet(input_size, hidden_size, output_size)                         # 需要是同一个模型\n",
        "model.load_state_dict(torch.load('model_weights.pth'))\n",
        "model.eval()\n",
        "\n",
        "'''\n",
        "be sure to call model.eval() method before inferencing to set the dropout and batch normalization layers to evaluation mode. \n",
        "Failing to do this will yield inconsistent inference results.\n",
        "'''"
      ],
      "metadata": {
        "id": "nQejY8J8ay0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 直接保存/加载整个模型\n",
        "torch.save(model, 'model.pth')"
      ],
      "metadata": {
        "id": "u0CxBdcFdJkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('model.pth')"
      ],
      "metadata": {
        "id": "7HujEzt6d9k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PISkr-lNd_iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discoveries\n",
        "1. SimpleNet的训练情况似乎说明**模型并不能从数据中学到什么东西（特征可能没有提供什么信息）**。"
      ],
      "metadata": {
        "id": "6GSfZNi978Mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AXbkrbORciD-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}