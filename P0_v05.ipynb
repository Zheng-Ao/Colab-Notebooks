{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P0_v05.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fKdAJ8l5F_yg",
        "VNIQr9XuakxG",
        "6GSfZNi978Mu"
      ],
      "mount_file_id": "1Sqr4KjJKKhLBjnKrl_PrfHPUOMsYKDXX",
      "authorship_tag": "ABX9TyO0zZGtcdMHfzafr/56wome",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zheng-Ao/Colab-Notebooks/blob/main/P0_v05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 全局设置"
      ],
      "metadata": {
        "id": "fKdAJ8l5F_yg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "CR_-xEl5Emv2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e031238-3726-49b3-a3c8-d15d89a63e9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.12.0+cu113\n",
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "'''常用库导入'''\n",
        "import random\n",
        "random.seed(850416)\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "raw_data_path = \"drive/MyDrive/P0/T10K.csv\"\n",
        "# 测试谷歌云端硬盘是否成功加载:\n",
        "# pd.read_csv(raw_data_path)\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "import torch\n",
        "print(torch.__version__)                                                        # 用于确定PyG的安装\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, Subset, ConcatDataset, DataLoader\n",
        "\n",
        "# 检查GPU:\n",
        "# !nvidia-smi\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "'''特殊库安装(魔法函数要放在Cell的最开始)'''\n",
        "# 我不想输出安装信息，虽然%%capture不是用来干这个的，但它能达到我想要的效果\n",
        "!pip install transformers\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "id": "q_Zxt-SIFo9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTILS.py"
      ],
      "metadata": {
        "id": "F_Rn1K8pE3Z0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Txt Embedding"
      ],
      "metadata": {
        "id": "X_w0ItLna28l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''构建txt_vecs，作为Dataset的第二个数据来源'''\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")      \n",
        "nlp_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
        "'''加载BERT这种大模型很耗时，因此在整个流程中应当让上面两行代码只执行一次。'''"
      ],
      "metadata": {
        "id": "Fc3zwgx_WRXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(raw_data_path)\n",
        "\n",
        "'''此循环较贵，但无法避免'''\n",
        "for i in tqdm(range(10000)):\n",
        "    ttl = df[\"patent_title\"].values[i]\n",
        "    inputs = tokenizer(ttl, return_tensors=\"pt\").to(device)\n",
        "    outputs = nlp_model(**inputs)\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "    cls_vec = last_hidden_states[:,0,:].clone().detach()\n",
        "    if i == 0:\n",
        "        txt_vecs = cls_vec\n",
        "    else:\n",
        "        txt_vecs = torch.cat([txt_vecs, cls_vec],dim=0)\n",
        "\n",
        "print(txt_vecs.shape)\n",
        "# should be [num_samples, embedding_dim] = [10000, 768]"
      ],
      "metadata": {
        "id": "TjwGItGlE5Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Mat = txt_vecs.cpu().numpy()\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# txt_pca = PCA(n_components=0.9, svd_solver = 'full')\n",
        "txt_pca = PCA(n_components = 5)"
      ],
      "metadata": {
        "id": "E_SqxF1chNgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 查看PCA的效果:\n",
        "txt_pca.fit(Mat)\n",
        "var = txt_pca.explained_variance_ratio_\n",
        "len(var), var.sum()"
      ],
      "metadata": {
        "id": "CaoLuPwphNVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Txt_Embedding = txt_pca.fit_transform(Mat)\n",
        "print(Txt_Embedding.shape)\n",
        "# should be [10000, 5] here"
      ],
      "metadata": {
        "id": "Wp1-4ZfciG91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## utils"
      ],
      "metadata": {
        "id": "eFTDdD4jgSP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "# target transform, 0:neg, 1:pos\n",
        "def LabelCorePa(ref, isd):\n",
        "    now = datetime.strptime(\"2022-01-01\", \"%Y-%m-%d\").year\n",
        "    years = now - datetime.strptime(isd, \"%Y-%m-%d\").year\n",
        "    score = ref/years       \n",
        "    label = int((score>0.5))\n",
        "    return label\n",
        "\n",
        "# Dataset\n",
        "class PatDataset(Dataset):\n",
        "    def __init__(self, raw_data_path, txt_vecs, transform = None, target_transform = LabelCorePa):\n",
        "        self.raw_data = pd.read_csv(raw_data_path)\n",
        "        self.txt_vecs = txt_vecs\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.raw_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Y          \n",
        "        ref = self.raw_data.at[idx, \"patent_num_cited_by_us_patents\"]\n",
        "        isd = self.raw_data.at[idx, \"patent_date\"]\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(ref, isd)\n",
        "            label = torch.tensor(label,dtype=torch.long)\n",
        "\n",
        "        # X\n",
        "        # INDEXs\n",
        "        num_claims = self.raw_data.at[idx, \"patent_num_claims\"]\n",
        "        num_b_cits = self.raw_data.at[idx, \"patent_num_us_patent_citations\"]\n",
        "        inventors = self.raw_data.at[idx, \"inventors\"]\n",
        "        num_inventors = len(eval(inventors))\n",
        "        assignees = self.raw_data.at[idx, \"assignees\"]\n",
        "        if assignees == \"[{'assignee_sequence': None, 'assignee_key_id': None}]\":\n",
        "            num_assignees = 0\n",
        "        else:\n",
        "            num_assignees = len(eval(assignees))\n",
        "        IPCs = self.raw_data.at[idx, \"IPCs\"]\n",
        "        num_ipcs = len(eval(IPCs))\n",
        "        indexs = torch.tensor([num_claims, num_b_cits, num_inventors, num_assignees, num_ipcs], dtype=torch.float32).to(device)\n",
        "        # TXT\n",
        "        txt_vec = torch.tensor(self.txt_vecs[idx], dtype = torch.float32)\n",
        "        txt_vec = txt_vec.to(device)\n",
        "\n",
        "        patent = torch.cat([indexs, txt_vec])\n",
        "        \n",
        "        return patent, label\n",
        "\n",
        "# Models\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            # nn.BatchNorm1d(input_size),\n",
        "            # 不加BatchNorm效果更好，事实证明：不要去玩自己不会的东西\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, output_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "from torch_geometric.nn import GCNConv\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_size, hidden_size)\n",
        "        self.conv2 = GCNConv(hidden_size, 2)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        logits = self.conv2(x, edge_index)                   # (num_nodes, 2)\n",
        "        return logits                  # (num_nodes, 2)\n",
        "\n",
        "\n",
        "\n",
        "# Train\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        # batch: 第几个batch；X: 包含batch_size个feature vec.\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        output = model(X).to(device)\n",
        "        loss = loss_fn(output, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 20 == 0:                                                     # train_size/batch_size = 100, 每20个batch输出一次结果，共输出5次。\n",
        "            loss, current = loss.item(), batch * len(X)                         \n",
        "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "# Test\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            output = model(X)\n",
        "            # test_loss += loss_fn(output, y).item()\n",
        "            # correct += (output.argmax(1) == y).type(torch.float).sum().item()\n",
        "            \n",
        "            pred = output.argmax(1).cpu()\n",
        "            y = y.cpu()\n",
        "            if batch == 0:\n",
        "                Pred = pred\n",
        "                Y = y\n",
        "            else:\n",
        "                Pred = torch.cat((Pred, pred), dim = 0)\n",
        "                Y = torch.cat((Y, y), dim=0)\n",
        "\n",
        "    C_Mat = metrics.confusion_matrix(Y, Pred)\n",
        "    accuracy = metrics.accuracy_score(Y,Pred)\n",
        "    f1 = metrics.f1_score(Y, Pred)\n",
        "    recall = metrics.recall_score(Y, Pred)\n",
        "    precision = metrics.precision_score(Y, Pred)\n",
        "    print(C_Mat)\n",
        "    print(f\"acc:{accuracy:.4f}, f1:{f1:.4f}, recall:{recall:.4f}, prec:{precision:.4f}\")\n",
        "\n",
        "    # test_loss /= num_batches\n",
        "    # correct /= size\n",
        "    # print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "K8tSi8TZFQyB"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# config.py"
      ],
      "metadata": {
        "id": "wmFzeBC6qaHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HyperParams----Config.py\n",
        "hidden_size = 16\n",
        "output_size = 2\n",
        "\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 5e-4\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "num_epochs_to_print = epochs/10                                                 # 每隔10次输出一次Metrics\n",
        "\n",
        "num_train = 8000\n",
        "num_test = 2000"
      ],
      "metadata": {
        "id": "jKlYsn2aqdjS"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "DgQRy866G5JW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Net"
      ],
      "metadata": {
        "id": "m0KrE7E6LDCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PIPLINE\n",
        "# DATA TO FIT A MODEL\n",
        "dataset = PatDataset(raw_data_path=raw_data_path, txt_vecs = Txt_Embedding)\n",
        "training_indices = [i for i in range(num_train)]\n",
        "test_indices = [i for i in range(num_train,num_train+num_test)]\n",
        "training_data_all = Subset(dataset, training_indices)\n",
        "test_data = Subset(dataset, test_indices)\n",
        "\n",
        "# Make pos:neg in training set 1:1\n",
        "pos_indices = []\n",
        "neg_indices = []\n",
        "for i in range(num_train):\n",
        "    if training_data_all[i][1] == 1:\n",
        "        pos_indices.append(i)\n",
        "    else:\n",
        "        neg_indices.append(i)\n",
        "num_pos = len(pos_indices)\n",
        "num_neg = len(neg_indices)\n",
        "print(f\"NEG:POS = {num_neg}:{num_pos} = {num_neg/num_pos:.2f}\")\n",
        "neg_indices_sample = np.random.choice(neg_indices, num_pos, replace = False)\t \n",
        "training_data_pos = Subset(training_data_all, pos_indices)\n",
        "training_data_neg = Subset(training_data_all, neg_indices_sample)\n",
        "training_data = ConcatDataset([training_data_pos, training_data_neg])\n",
        "print(\"Training sample number:\",len(training_data))\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")"
      ],
      "metadata": {
        "id": "Cnjn-MyeOvYq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e753eed-097e-4ca0-dfbb-a84fe7707407"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NEG:POS = 6633:1367 = 4.85\n",
            "Training sample number: 2734\n",
            "Feature batch shape: torch.Size([64, 10])\n",
            "Labels batch shape: torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FIT A MODEL\n",
        "# PyTorch的逻辑是先初始化（喂超参），再进行函数计算（喂输入）\n",
        "input_size = len(dataset[0][0])\n",
        "model = SimpleNet(input_size, hidden_size, output_size).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "for t in range(epochs):\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    if t==0 or (t+1)%num_epochs_to_print == 0:\n",
        "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "        test_loop(train_dataloader, model, loss_fn)                                 # performance on training data\n",
        "        test_loop(test_dataloader, model, loss_fn)                                  # performance on test data\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "iZiFY_jDIhNQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5d2e4a2-62b2-4f3f-c006-524521ba4ef1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "[[ 295 1072]\n",
            " [ 341 1026]]\n",
            "acc:0.4832, f1:0.5922, recall:0.7505, prec:0.4890\n",
            "[[ 378 1181]\n",
            " [ 112  329]]\n",
            "acc:0.3535, f1:0.3373, recall:0.7460, prec:0.2179\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "[[653 714]\n",
            " [426 941]]\n",
            "acc:0.5830, f1:0.6228, recall:0.6884, prec:0.5686\n",
            "[[701 858]\n",
            " [120 321]]\n",
            "acc:0.5110, f1:0.3963, recall:0.7279, prec:0.2723\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "[[820 547]\n",
            " [563 804]]\n",
            "acc:0.5940, f1:0.5916, recall:0.5881, prec:0.5951\n",
            "[[887 672]\n",
            " [174 267]]\n",
            "acc:0.5770, f1:0.3870, recall:0.6054, prec:0.2843\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "[[783 584]\n",
            " [523 844]]\n",
            "acc:0.5951, f1:0.6039, recall:0.6174, prec:0.5910\n",
            "[[843 716]\n",
            " [153 288]]\n",
            "acc:0.5655, f1:0.3986, recall:0.6531, prec:0.2869\n",
            "Epoch 40\n",
            "-------------------------------\n",
            "[[664 703]\n",
            " [430 937]]\n",
            "acc:0.5856, f1:0.6232, recall:0.6854, prec:0.5713\n",
            "[[696 863]\n",
            " [118 323]]\n",
            "acc:0.5095, f1:0.3970, recall:0.7324, prec:0.2723\n",
            "Epoch 50\n",
            "-------------------------------\n",
            "[[870 497]\n",
            " [593 774]]\n",
            "acc:0.6013, f1:0.5868, recall:0.5662, prec:0.6090\n",
            "[[920 639]\n",
            " [180 261]]\n",
            "acc:0.5905, f1:0.3893, recall:0.5918, prec:0.2900\n",
            "Epoch 60\n",
            "-------------------------------\n",
            "[[924 443]\n",
            " [644 723]]\n",
            "acc:0.6024, f1:0.5709, recall:0.5289, prec:0.6201\n",
            "[[972 587]\n",
            " [200 241]]\n",
            "acc:0.6065, f1:0.3798, recall:0.5465, prec:0.2911\n",
            "Epoch 70\n",
            "-------------------------------\n",
            "[[822 545]\n",
            " [544 823]]\n",
            "acc:0.6017, f1:0.6018, recall:0.6020, prec:0.6016\n",
            "[[855 704]\n",
            " [160 281]]\n",
            "acc:0.5680, f1:0.3941, recall:0.6372, prec:0.2853\n",
            "Epoch 80\n",
            "-------------------------------\n",
            "[[792 575]\n",
            " [520 847]]\n",
            "acc:0.5995, f1:0.6074, recall:0.6196, prec:0.5956\n",
            "[[816 743]\n",
            " [149 292]]\n",
            "acc:0.5540, f1:0.3957, recall:0.6621, prec:0.2821\n",
            "Epoch 90\n",
            "-------------------------------\n",
            "[[808 559]\n",
            " [530 837]]\n",
            "acc:0.6017, f1:0.6059, recall:0.6123, prec:0.5996\n",
            "[[826 733]\n",
            " [152 289]]\n",
            "acc:0.5575, f1:0.3951, recall:0.6553, prec:0.2828\n",
            "Epoch 100\n",
            "-------------------------------\n",
            "[[809 558]\n",
            " [532 835]]\n",
            "acc:0.6013, f1:0.6051, recall:0.6108, prec:0.5994\n",
            "[[837 722]\n",
            " [150 291]]\n",
            "acc:0.5640, f1:0.4003, recall:0.6599, prec:0.2873\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyG GNN"
      ],
      "metadata": {
        "id": "HZwU5sBFyly0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# edge_index, edge_attr\n",
        "df = pd.read_csv(raw_data_path)\n",
        "\n",
        "adj_ls = []\n",
        "edges = []\n",
        "\n",
        "for i in tqdm(range(10000)):\n",
        "    id = df.values[i][0]\n",
        "    b_cits = eval(df[\"cited_patents\"].values[i])\n",
        "    for b_cit in b_cits:\n",
        "        # 是否在专利数据库中\n",
        "        if b_cit[\"cited_patent_date\"]:\n",
        "            # 是否在本数据集中\n",
        "            if b_cit[\"cited_patent_number\"] in df[\"patent_number\"].values:\n",
        "                # print(i)\n",
        "                cit_id = df[df[\"patent_number\"].values == b_cit[\"cited_patent_number\"]].values[0][0]\n",
        "                adj_ls.append([id, cit_id])\n",
        "                date = df[\"patent_date\"].values[id]\n",
        "                cit_date = df[\"patent_date\"].values[cit_id]\n",
        "                date1 = datetime.strptime(date, \"%Y-%m-%d\")\n",
        "                date2 = datetime.strptime(cit_date, \"%Y-%m-%d\")\n",
        "                dist = date1-date2\n",
        "                dist = 365/dist.days\n",
        "                edges.append(dist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-myLjEp_2AHk",
        "outputId": "099c8047-81d9-4193-feff-2eb4cef9d343"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:43<00:00, 231.73it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df[df[\"patent_number\"].values == '3930276'].values[0][0]\n",
        "# \"3930276\" in df[\"patent_number\"].values\n",
        "# edges"
      ],
      "metadata": {
        "id": "Xxur-qrM2SnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data.x, data.y\n",
        "for i in range(10000):\n",
        "    if i == 0:\n",
        "        x = dataset[i][0].unsqueeze(0)\n",
        "        y = dataset[i][1].unsqueeze(0)\n",
        "    else:\n",
        "        x = torch.cat([x, dataset[i][0].unsqueeze(0)])\n",
        "        y = torch.cat([y, dataset[i][1].unsqueeze(0)])\n",
        "\n",
        "# "
      ],
      "metadata": {
        "id": "JT6lZqVrIsnt"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape, x.dtype, y.shape, y.dtype"
      ],
      "metadata": {
        "id": "CtjHVuhNP3vn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afdf5659-5d4d-44cd-edf0-5d775ecc0b35"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10000, 10]), torch.float32, torch.Size([10000]), torch.int64)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(pos_indices),len(neg_indices_sample))\n",
        "print(neg_indices_sample)\n",
        "# len = 10\n",
        "# indices = [1,2,4,5]\n",
        " \n",
        "# ls1 = [False]*10\n",
        "# t = torch.tensor(ls1)\n",
        "# t[indices] = True\n",
        "# t\n",
        "\n",
        "# for i in range(len(pos_indices)):\n",
        "#     if pos_indices[i] == neg_indices_sample[i]:\n",
        "#         print(i)\n",
        "\n",
        "ls = [False]*10000\n",
        "\n",
        "# ls[pos_indices] = True\n",
        "\n",
        "ls_t = torch.tensor(ls, dtype = bool)\n",
        "# ls_t[pos_indices] = True\n",
        "ls_t[neg_indices_sample] = True\n",
        "ls_t.sum()\n"
      ],
      "metadata": {
        "id": "7gcVPUeMMurI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data\n",
        "\n",
        "# num_nodes, num_node_features\n",
        "x = torch.tensor(x, dtype=torch.float32)\n",
        "\n",
        "# num_nodes, 1\n",
        "y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# 2, num_edges\n",
        "edge_index = torch.tensor(adj_ls, dtype=torch.long)\n",
        "\n",
        "# num_edges, num_edge_features\n",
        "edge_attr = torch.tensor(edges, dtype=torch.float32)\n",
        "\n",
        "# train-test split\n",
        "train_ls = [False]*num_train + [False]*num_test\n",
        "train_mask = torch.tensor(train_ls, dtype=bool)\n",
        "train_mask[pos_indices] = True\n",
        "train_mask[neg_indices_sample] = True\n",
        "test_ls = [False]*num_train + [True]*num_test\n",
        "test_mask = torch.tensor(test_ls, dtype=bool)\n",
        "\n",
        "\n",
        "data = Data(x=x, y=y, edge_index = edge_index.t().contiguous(), edge_attr=edge_attr, train_mask = train_mask, test_mask=test_mask)\n",
        "data = data.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXfkKRT6CGHV",
        "outputId": "7e307f6e-a96d-439e-dd41-96e4e3272947"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of nodes: {data.num_nodes}') # 节点数量\n",
        "print(f'Number of edges: {data.num_edges}') # 边数量\n",
        "print(f'Number of node features: {data.num_node_features}') # 节点属性的维度\n",
        "print(f'Number of node features: {data.num_features}') # 同样是节点属性的维度\n",
        "print(f'Number of edge features: {data.num_edge_features}') # 边属性的维度\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}') # 平均节点度\n",
        "print(f'if edge indices are ordered and do not contain duplicate entries.: {data.is_coalesced()}') # 是否边是有序的同时不含有重复的边\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}') # 用作训练集的节点\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}') # 用作训练集的节点的数量\n",
        "print(f'Contains isolated nodes: {data.has_isolated_nodes()}') # 此图是否包含孤立的节点\n",
        "print(f'Contains self-loops: {data.has_self_loops()}')  # 此图是否包含自环的边\n",
        "print(f'Is undirected: {data.is_undirected()}')  # 此图是否是无向图"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfxaz5LtCGFC",
        "outputId": "aea92517-9d1a-4c83-8e09-c063367369aa"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of nodes: 10000\n",
            "Number of edges: 5561\n",
            "Number of node features: 10\n",
            "Number of node features: 10\n",
            "Number of edge features: 1\n",
            "Average node degree: 0.56\n",
            "if edge indices are ordered and do not contain duplicate entries.: False\n",
            "Number of training nodes: 2734\n",
            "Training node label rate: 0.27\n",
            "Contains isolated nodes: True\n",
            "Contains self-loops: False\n",
            "Is undirected: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = len(dataset[0][0])\n",
        "\n",
        "model = GNN(input_size, hidden_size, output_size).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "model.train()\n",
        "for epoch in range(200):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data)\n",
        "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "cCx5_Hl5CGCZ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "pred = model(data).argmax(dim=1)\n",
        "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
        "acc = int(correct) / int(data.test_mask.sum())\n",
        "print(f'Accuracy: {acc:.4f}')\n",
        "Pred = pred[data.test_mask].cpu()\n",
        "Y = data.y[data.test_mask].cpu()\n",
        "C_Mat = metrics.confusion_matrix(Y, Pred)\n",
        "accuracy = metrics.accuracy_score(Y,Pred)\n",
        "f1 = metrics.f1_score(Y, Pred)\n",
        "recall = metrics.recall_score(Y, Pred)\n",
        "precision = metrics.precision_score(Y, Pred)\n",
        "print(C_Mat)\n",
        "print(f\"acc:{accuracy:.4f}, f1:{f1:.4f}, recall:{recall:.4f}, prec:{precision:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D44a9N5TCF6D",
        "outputId": "88687559-1aaf-4907-d0f1-790630f60faa"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6235\n",
            "[[1026  533]\n",
            " [ 220  221]]\n",
            "acc:0.6235, f1:0.3699, recall:0.5011, prec:0.2931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save&Load Model"
      ],
      "metadata": {
        "id": "VNIQr9XuakxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 保存模型权重至当前文件夹\n",
        "torch.save(model.state_dict(), 'model_weights.pth')                             "
      ],
      "metadata": {
        "id": "tTO_IOiiarkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleNet(input_size, hidden_size, output_size)                         # 需要是同一个模型\n",
        "model.load_state_dict(torch.load('model_weights.pth'))\n",
        "model.eval()\n",
        "\n",
        "'''\n",
        "be sure to call model.eval() method before inferencing to set the dropout and batch normalization layers to evaluation mode. \n",
        "Failing to do this will yield inconsistent inference results.\n",
        "'''"
      ],
      "metadata": {
        "id": "nQejY8J8ay0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 直接保存/加载整个模型\n",
        "torch.save(model, 'model.pth')"
      ],
      "metadata": {
        "id": "u0CxBdcFdJkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('model.pth')"
      ],
      "metadata": {
        "id": "7HujEzt6d9k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PISkr-lNd_iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discoveries\n",
        "1. SimpleNet的训练情况似乎说明**模型并不能从数据中学到什么东西（特征可能没有提供什么信息）**。"
      ],
      "metadata": {
        "id": "6GSfZNi978Mu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. GNN的训练好像比SimpleNet快很多，可能是train_loop写得太复杂了。"
      ],
      "metadata": {
        "id": "NzEJ90CXhX_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AXbkrbORciD-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}