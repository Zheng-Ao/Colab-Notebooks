{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P0_v05.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fKdAJ8l5F_yg",
        "C-W-OYm27Tj2",
        "mgVZ4TC0Vda_",
        "HxffvHLxQbHI",
        "N0f4YlrVQxQv",
        "wmFzeBC6qaHX",
        "DgQRy866G5JW",
        "HZwU5sBFyly0",
        "VNIQr9XuakxG",
        "6GSfZNi978Mu"
      ],
      "mount_file_id": "1Sqr4KjJKKhLBjnKrl_PrfHPUOMsYKDXX",
      "authorship_tag": "ABX9TyMsY1qEsCDMxrPwxw3nRn+a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zheng-Ao/Colab-Notebooks/blob/main/P0_v05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 全局设置"
      ],
      "metadata": {
        "id": "fKdAJ8l5F_yg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CR_-xEl5Emv2"
      },
      "outputs": [],
      "source": [
        "'''常用库导入'''\n",
        "import random\n",
        "random.seed(85)\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "raw_data_path = \"drive/MyDrive/P0/T10K.csv\"\n",
        "# 测试谷歌云端硬盘是否成功加载:\n",
        "# pd.read_csv(raw_data_path)\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "import torch\n",
        "print(torch.__version__)                                                        # 用于确定PyG的安装\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, Subset, ConcatDataset, DataLoader\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# 检查GPU:\n",
        "# !nvidia-smi\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "'''特殊库安装(魔法函数要放在Cell的最开始)'''\n",
        "# 我不想输出安装信息，虽然%%capture不是用来干这个的，但它能达到我想要的效果\n",
        "!pip install transformers\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
        "!pip install torch-geometric"
      ],
      "metadata": {
        "id": "q_Zxt-SIFo9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 数据探索，预处理，特征工程"
      ],
      "metadata": {
        "id": "C-W-OYm27Tj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 原始数据->分析数据"
      ],
      "metadata": {
        "id": "UKYQ1QPPWJ_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data = pd.read_csv(raw_data_path, index_col=\"Unnamed: 0\")                   # 去掉自带的Unnamed: 0的index列\n",
        "\n",
        "# 将3个字符串变成数值\n",
        "for str in [\"inventors\",\"assignees\",\"IPCs\"]:\n",
        "    for i in range(len(raw_data[str])):\n",
        "        if str == \"assignees\":\n",
        "            if raw_data[str].values[i] == \"[{'assignee_sequence': None, 'assignee_key_id': None}]\":\n",
        "                raw_data[str].values[i] = 0\n",
        "                continue\n",
        "        raw_data[str].values[i] = len(eval(raw_data[str].values[i]))\n",
        "\n",
        "# 上述操作虽然将值改了，但dtype仍然是object\n",
        "for str in [\"inventors\",\"assignees\",\"IPCs\"]:\n",
        "    raw_data[str] = raw_data[str].astype(int)\n",
        "\n",
        "# raw_data.info()\n",
        "# raw_data.describe()"
      ],
      "metadata": {
        "id": "cpDvnTztV_VQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 数据清洗"
      ],
      "metadata": {
        "id": "-ofPd_sYUAty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''本次清洗原则：低被引，皆可删\n",
        "注意，清洗之后要重排id列，这是建图的必须！\n",
        "默认的索引也要重新改！否则无法正常的进行索引操作！比如原来index=2800的被删掉了，那么df.at[2800,\"someattr\"]就会报错！！！\n",
        "'''\n",
        "\n",
        "# raw_data.info()\n",
        "# 发现\"patent_abstract\"一列有缺失值，定位看看:\n",
        "raw_data[raw_data.isnull().values == True]\n",
        "# 如果无关紧要的负样本，直接删掉:\n",
        "raw_data.dropna(inplace = True)\n",
        "raw_data.reset_index(drop=True, inplace=True)\n",
        "# raw_data.info()\n",
        "# raw_data.describe()\n",
        "# 发现后向引用数的min为0，存在没有后向引用的专利吗？定位看看:\n",
        "# raw_data[raw_data[\"patent_num_us_patent_citations\"].values == 0]\n",
        "# 这些专利并非后向引用为0，而是引用了外国的专利，而非美国的专利。\n",
        "# raw_data[raw_data[\"patent_num_us_patent_citations\"].values == 1]\n",
        "# 这些专利还是有一定被引数的，因此保留下来\n",
        "num_patents = len(raw_data)\n",
        "raw_data.insert(0, 'index', range(num_patents), allow_duplicates=False)         # 提供id列，方便建图\n",
        "\n",
        "print(f\"最终有{num_patents}个专利\")"
      ],
      "metadata": {
        "id": "ON6lV9St7ZWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# raw_data"
      ],
      "metadata": {
        "id": "i69vMzfKkNX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 看一看正负样本分别长什么样"
      ],
      "metadata": {
        "id": "mgVZ4TC0Vda_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# raw_data.nlargest(10,\"patent_num_cited_by_us_patents\")\n",
        "pos_some_data = raw_data[raw_data[\"patent_num_cited_by_us_patents\"]>40]\n",
        "pos_some_data.info()\n",
        "pos_some_data.describe()"
      ],
      "metadata": {
        "id": "b_MsEyGb9tLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**可以看到基本上没什么规律，除非能够很好地理解文本，否则很难自动地将它们挑出来。**"
      ],
      "metadata": {
        "id": "g3-M93QIaXeg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 分析数据->输入特征"
      ],
      "metadata": {
        "id": "I4zDxm3sb00q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 数值特征归一化"
      ],
      "metadata": {
        "id": "hHq4fFKlgLVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for str in [\"patent_num_claims\",\"patent_num_us_patent_citations\",\"inventors\",\"assignees\",\"IPCs\"]:\n",
        "    raw_data[str] = (raw_data[str] - raw_data[str].mean())/raw_data[str].std()"
      ],
      "metadata": {
        "id": "UjB4DHL1Wyb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# raw_data"
      ],
      "metadata": {
        "id": "S3BmMYvS9tJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 文本特征提取"
      ],
      "metadata": {
        "id": "_87Ekpruce4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nlp_model = DistillBert\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")      \n",
        "nlp_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)"
      ],
      "metadata": {
        "id": "Fc3zwgx_WRXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(num_patents)):\n",
        "    ttl = raw_data[\"patent_title\"].values[i]\n",
        "    abst = raw_data[\"patent_abstract\"].values[i]\n",
        "    txt_input = ttl + \" \" + abst\n",
        "    # txt_input = ttl\n",
        "    inputs = tokenizer(txt_input, return_tensors=\"pt\").to(device)\n",
        "    if len(inputs['input_ids'][0]) > 512:\n",
        "        # 取前128和后382个token作为输入，详见Bert输入\n",
        "        head_input = inputs['input_ids'][0, :129]\n",
        "        tail_input = inputs['input_ids'][0, -383:]\n",
        "        head_mask = inputs['attention_mask'][0, :129]\n",
        "        tail_mask = inputs['attention_mask'][0, -383:]\n",
        "        input_ids = torch.cat([head_input, tail_input]).unsqueeze(0)\n",
        "        attention_mask = torch.cat([head_mask, tail_mask]).unsqueeze(0)\n",
        "        outputs = nlp_model(input_ids = input_ids, attention_mask = attention_mask)\n",
        "    else:\n",
        "        # inputs是一个字典，因此要用**kwargs的形式\n",
        "        outputs = nlp_model(**inputs)\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "    # 详见Bert模型介绍\n",
        "    cls_vec = last_hidden_states[:,0,:].clone().detach()\n",
        "    if i == 0:\n",
        "        txt_vecs = cls_vec\n",
        "    else:\n",
        "        txt_vecs = torch.cat([txt_vecs, cls_vec],dim=0)\n",
        "\n",
        "print(txt_vecs.shape)\n",
        "# should be [num_samples, embedding_dim]"
      ],
      "metadata": {
        "id": "TjwGItGlE5Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Mat = txt_vecs.cpu().numpy()\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# txt_pca = PCA(n_components=0.9, svd_solver = 'full')\n",
        "txt_pca = PCA(n_components = 5)"
      ],
      "metadata": {
        "id": "E_SqxF1chNgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 查看PCA的效果:\n",
        "txt_pca.fit(Mat)\n",
        "var = txt_pca.explained_variance_ratio_\n",
        "print(f\"PCA之后的维度：{len(var)}，PCA后的方差保留：{var.sum():.4f}\")"
      ],
      "metadata": {
        "id": "CaoLuPwphNVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA\n",
        "Txt_Embedding_c = torch.tensor(txt_pca.fit_transform(Mat), dtype = torch.float32)\n",
        "print(Txt_Embedding_c.shape)\n",
        "\n",
        "# without PCA\n",
        "Txt_Embedding = txt_vecs.clone()\n",
        "print(Txt_Embedding.shape, Txt_Embedding.dtype)"
      ],
      "metadata": {
        "id": "Wp1-4ZfciG91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 图特征提取"
      ],
      "metadata": {
        "id": "WLuD8FMHdsIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# edge_index（邻接列表）, edge_attr（边特征）\n",
        "from datetime import datetime\n",
        "\n",
        "adj_ls = []\n",
        "edges = []\n",
        "\n",
        "'''这里有两个for循环，O(N*L)，非常贵！暂时没有其它手法，因此暂时不要处理10K级别以上的数据。'''\n",
        "for i in tqdm(range(num_patents)):\n",
        "    id = raw_data.values[i][0]\n",
        "    b_cits = eval(raw_data[\"cited_patents\"].values[i])\n",
        "    for b_cit in b_cits:\n",
        "        # 是否在专利数据库中（b_cit[\"cited_patent_date\"] != None）\n",
        "        if b_cit[\"cited_patent_date\"]:\n",
        "            # 是否在本数据集中\n",
        "            if b_cit[\"cited_patent_number\"] in raw_data[\"patent_number\"].values:\n",
        "                cit_id = raw_data[raw_data[\"patent_number\"].values == b_cit[\"cited_patent_number\"]].values[0][0]\n",
        "                adj_ls.append([id, cit_id])\n",
        "                date = raw_data[\"patent_date\"].values[id]\n",
        "                cit_date = raw_data[\"patent_date\"].values[cit_id]\n",
        "                date1 = datetime.strptime(date, \"%Y-%m-%d\")\n",
        "                date2 = datetime.strptime(cit_date, \"%Y-%m-%d\")\n",
        "                dist = date1-date2\n",
        "                '''边的权重设置为365/间隔天数'''\n",
        "                dist = 365/dist.days\n",
        "                edges.append(dist)"
      ],
      "metadata": {
        "id": "JXWJMF7BPFT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adj_ls[0:10], edges[0:10]\n",
        "'''edges需要做归一化吗？'''"
      ],
      "metadata": {
        "id": "Jm2nU3jQPj3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UTILS"
      ],
      "metadata": {
        "id": "F_Rn1K8pE3Z0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "HxffvHLxQbHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0:neg, 1:pos\n",
        "def LabelCorePa(ref, isd):\n",
        "    now = datetime.strptime(\"2022-01-01\", \"%Y-%m-%d\").year\n",
        "    years = now - datetime.strptime(isd, \"%Y-%m-%d\").year\n",
        "    score = ref/years       \n",
        "    label = int((score>0.5))\n",
        "    return label\n",
        "\n",
        "class PatDataset(Dataset):\n",
        "    def __init__(self, raw_data, txt_vecs = None, transform = None, target_transform = LabelCorePa):\n",
        "        super().__init__()\n",
        "        self.raw_data = raw_data\n",
        "        self.txt_vecs = txt_vecs\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.raw_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Y          \n",
        "        ref = self.raw_data[\"patent_num_cited_by_us_patents\"].values[idx]\n",
        "        isd = self.raw_data.at[idx, \"patent_date\"]\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(ref, isd)\n",
        "            label = torch.tensor(label,dtype=torch.long)\n",
        "\n",
        "        # X\n",
        "        # INDEXs\n",
        "        num_claims = self.raw_data.at[idx, \"patent_num_claims\"]\n",
        "        num_b_cits = self.raw_data.at[idx, \"patent_num_us_patent_citations\"]\n",
        "        inventors = self.raw_data.at[idx, \"inventors\"]\n",
        "        assignees = self.raw_data.at[idx, \"assignees\"]\n",
        "        IPCs = self.raw_data.at[idx, \"IPCs\"]\n",
        "        indexs = torch.tensor([num_claims, num_b_cits, inventors, assignees, IPCs], dtype=torch.float32).to(device)\n",
        "        patent = indexs.clone()\n",
        "        # TXT\n",
        "        if self.txt_vecs != None:\n",
        "            txt_vec = self.txt_vecs[idx].clone()\n",
        "            txt_vec = txt_vec.to(device)\n",
        "            patent = torch.cat([indexs, txt_vec])\n",
        "        \n",
        "        return patent, label"
      ],
      "metadata": {
        "id": "K8tSi8TZFQyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Net Modules"
      ],
      "metadata": {
        "id": "yXkXnL0wQe2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def TowerBlock(input_size, output_size):\n",
        "    block = nn.Sequential(\n",
        "        nn.Linear(input_size, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.Linear(128, 32),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.Linear(32, 8),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(p=0.5),\n",
        "        nn.Linear(8, output_size)\n",
        "    )\n",
        "    return block\n",
        "\n",
        "class MLPNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            # nn.BatchNorm1d(input_size),\n",
        "            # 不加BatchNorm效果更好，事实证明：不要去玩自己不会的东西\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            # TowerBlock(hidden_size, output_size)\n",
        "            nn.Linear(hidden_size, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "class GNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.conv1 = GCNConv(input_size, hidden_size)\n",
        "        self.conv2 = GCNConv(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, training=self.training)\n",
        "        logits = self.conv2(x, edge_index)                   \n",
        "        return logits                  "
      ],
      "metadata": {
        "id": "G8sD8elTQVCf"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-Test Loop"
      ],
      "metadata": {
        "id": "N0f4YlrVQxQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # batch: 第几个batch；X: 包含batch_size个feature vec.\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "        output = model(X).to(device)\n",
        "        loss = loss_fn(output, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            output = model(X)\n",
        "            # torch crossentropy loss是output在前，target在后\n",
        "            test_loss += loss_fn(output, y).item()\n",
        "            pred = output.argmax(1).cpu()\n",
        "            y = y.cpu()\n",
        "            if batch == 0:\n",
        "                Pred = pred\n",
        "                Y = y\n",
        "            else:\n",
        "                Pred = torch.cat((Pred, pred), dim = 0)\n",
        "                Y = torch.cat((Y, y), dim=0)\n",
        "\n",
        "    avg_loss = test_loss/num_batches\n",
        "    print(f\"avg_loss:{avg_loss:.4f}\")\n",
        "    Performance(Y, Pred)\n",
        "\n",
        "def Performance(y_true, y_pred):\n",
        "    # y_true在前，y_pred在后\n",
        "    C_Mat = metrics.confusion_matrix(y_true, y_pred)\n",
        "    accuracy = metrics.accuracy_score(y_true,y_pred)\n",
        "    f1 = metrics.f1_score(y_true, y_pred)\n",
        "    recall = metrics.recall_score(y_true, y_pred)\n",
        "    precision = metrics.precision_score(y_true, y_pred)\n",
        "    print(C_Mat)\n",
        "    print(f\"acc:{accuracy:.4f}, f1:{f1:.4f}, recall:{recall:.4f}, prec:{precision:.4f}\")\n",
        "    "
      ],
      "metadata": {
        "id": "3PF4E5HoQnxK"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONFIG"
      ],
      "metadata": {
        "id": "wmFzeBC6qaHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HyperParams----Config.py\n",
        "hidden_size = 16\n",
        "output_size = 2\n",
        "\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 5e-4\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "num_epochs_to_print = epochs/10                                                 # 每隔10次输出一次Metrics\n",
        "\n",
        "num_train = 8000\n",
        "num_test = num_patents - num_train"
      ],
      "metadata": {
        "id": "jKlYsn2aqdjS"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "DgQRy866G5JW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = PatDataset(raw_data=raw_data, txt_vecs = Txt_Embedding)\n",
        "training_indices = [i for i in range(num_train)]\n",
        "test_indices = [i for i in range(num_train,num_train+num_test)]\n",
        "training_data_all = Subset(dataset, training_indices)\n",
        "test_data = Subset(dataset, test_indices)\n",
        "\n",
        "# Make pos:neg in training set 1:1\n",
        "pos_indices = []\n",
        "neg_indices = []\n",
        "for i in tqdm(range(num_train)):\n",
        "    if training_data_all[i][1] == 1:\n",
        "        pos_indices.append(i)\n",
        "    else:\n",
        "        neg_indices.append(i)\n",
        "num_pos = len(pos_indices)\n",
        "num_neg = len(neg_indices)\n",
        "print(f\"NEG:POS = {num_neg}:{num_pos} = {num_neg/num_pos:.2f}\")\n",
        "neg_indices_sample = np.random.choice(neg_indices, num_pos, replace = False)\t \n",
        "training_data_pos = Subset(training_data_all, pos_indices)\n",
        "training_data_neg = Subset(training_data_all, neg_indices_sample)\n",
        "training_data = ConcatDataset([training_data_pos, training_data_neg])\n",
        "print(\"Training sample number:\",len(training_data))\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")"
      ],
      "metadata": {
        "id": "Cnjn-MyeOvYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP"
      ],
      "metadata": {
        "id": "m0KrE7E6LDCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch的逻辑是先初始化（喂超参），再进行函数计算（喂输入）\n",
        "\n",
        "input_size = len(dataset[0][0])\n",
        "MLP = MLPNet(input_size, hidden_size, output_size).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\"\"\"nn.CrossEntropyLoss()会自动对loss进行batch内部的平均，即一个batch的总loss/batch_size。\n",
        "即输出的不是总loss，而是平均loss！\"\"\"\n",
        "optimizer = torch.optim.Adam(MLP.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "for t in range(epochs):\n",
        "    train_loop(train_dataloader, MLP, loss_fn, optimizer)\n",
        "    if t==0 or (t+1)%num_epochs_to_print == 0:\n",
        "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "        test_loop(train_dataloader, MLP, loss_fn)                               \n",
        "        test_loop(test_dataloader, MLP, loss_fn)                                \n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "iZiFY_jDIhNQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "54525423-79bd-4b16-adb5-676a7762f86c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "avg_loss:0.6813\n",
            "[[ 249 1120]\n",
            " [  68 1301]]\n",
            "acc:0.5661, f1:0.6865, recall:0.9503, prec:0.5374\n",
            "avg_loss:0.7158\n",
            "[[ 257 1297]\n",
            " [  32  407]]\n",
            "acc:0.3332, f1:0.3798, recall:0.9271, prec:0.2388\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "avg_loss:0.6214\n",
            "[[1088  281]\n",
            " [ 677  692]]\n",
            "acc:0.6501, f1:0.5909, recall:0.5055, prec:0.7112\n",
            "avg_loss:0.6051\n",
            "[[1116  438]\n",
            " [ 202  237]]\n",
            "acc:0.6789, f1:0.4255, recall:0.5399, prec:0.3511\n",
            "Epoch 20\n",
            "-------------------------------\n",
            "avg_loss:0.5966\n",
            "[[871 498]\n",
            " [370 999]]\n",
            "acc:0.6830, f1:0.6971, recall:0.7297, prec:0.6673\n",
            "avg_loss:0.6925\n",
            "[[849 705]\n",
            " [134 305]]\n",
            "acc:0.5790, f1:0.4210, recall:0.6948, prec:0.3020\n",
            "Epoch 30\n",
            "-------------------------------\n",
            "avg_loss:0.5812\n",
            "[[958 411]\n",
            " [425 944]]\n",
            "acc:0.6947, f1:0.6931, recall:0.6896, prec:0.6967\n",
            "avg_loss:0.6656\n",
            "[[930 624]\n",
            " [156 283]]\n",
            "acc:0.6086, f1:0.4205, recall:0.6446, prec:0.3120\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-da238e378454>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mnum_epochs_to_print\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-b408f18b6e69>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;31m# batch: 第几个batch；X: 包含batch_size个feature vec.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0msample_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumulative_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-0369336ea5d6>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mnum_claims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"patent_num_claims\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mnum_b_cits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"patent_num_us_patent_citations\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0minventors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inventors\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0massignees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"assignees\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mIPCs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IPCs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2273\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2275\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2221\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, index, col, takeable)\u001b[0m\n\u001b[1;32m   3566\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3568\u001b[0;31m         \u001b[0mseries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3569\u001b[0m         \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyG GNN"
      ],
      "metadata": {
        "id": "HZwU5sBFyly0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data.x, data.y\n",
        "for i in range(num_patents):\n",
        "    if i == 0:\n",
        "        x = dataset[i][0].unsqueeze(0)\n",
        "        y = dataset[i][1].unsqueeze(0)\n",
        "    else:\n",
        "        x = torch.cat([x, dataset[i][0].unsqueeze(0)])\n",
        "        y = torch.cat([y, dataset[i][1].unsqueeze(0)])\n",
        "\n",
        "print(x.shape, x.dtype, y.shape, y.dtype)"
      ],
      "metadata": {
        "id": "l_QEOKU8Pj6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(pos_indices),len(neg_indices_sample))\n",
        "print(neg_indices_sample)"
      ],
      "metadata": {
        "id": "7gcVPUeMMurI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Data\n",
        "\n",
        "# num_nodes, num_node_features\n",
        "x = torch.tensor(x, dtype=torch.float32)\n",
        "\n",
        "# num_nodes, 1\n",
        "y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "# 2, num_edges\n",
        "edge_index = torch.tensor(adj_ls, dtype=torch.long)\n",
        "\n",
        "# num_edges, num_edge_features\n",
        "edge_attr = torch.tensor(edges, dtype=torch.float32)\n",
        "\n",
        "# train-test split\n",
        "train_ls = [False]*num_train + [False]*num_test\n",
        "train_mask = torch.tensor(train_ls, dtype=bool)\n",
        "train_mask[pos_indices] = True\n",
        "train_mask[neg_indices_sample] = True\n",
        "test_ls = [False]*num_train + [True]*num_test\n",
        "test_mask = torch.tensor(test_ls, dtype=bool)\n",
        "\n",
        "\n",
        "data = Data(x=x, y=y, edge_index = edge_index.t().contiguous(), edge_attr=edge_attr, train_mask = train_mask, test_mask=test_mask)\n",
        "data = data.to(device)"
      ],
      "metadata": {
        "id": "fXfkKRT6CGHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of nodes: {data.num_nodes}') # 节点数量\n",
        "print(f'Number of edges: {data.num_edges}') # 边数量\n",
        "print(f'Number of node features: {data.num_node_features}') # 节点属性的维度\n",
        "print(f'Number of node features: {data.num_features}') # 同样是节点属性的维度\n",
        "print(f'Number of edge features: {data.num_edge_features}') # 边属性的维度\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}') # 平均节点度\n",
        "print(f'if edge indices are ordered and do not contain duplicate entries.: {data.is_coalesced()}') # 是否边是有序的同时不含有重复的边\n",
        "print(f'Number of training nodes: {data.train_mask.sum()}') # 用作训练集的节点\n",
        "print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.2f}') # 用作训练集的节点的数量\n",
        "print(f'Contains isolated nodes: {data.has_isolated_nodes()}') # 此图是否包含孤立的节点\n",
        "print(f'Contains self-loops: {data.has_self_loops()}')  # 此图是否包含自环的边\n",
        "print(f'Is undirected: {data.is_undirected()}')  # 此图是否是无向图"
      ],
      "metadata": {
        "id": "tfxaz5LtCGFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = len(dataset[0][0])\n",
        "\n",
        "Node2vec = GNN(input_size, hidden_size, output_size).to(device)\n",
        "optimizer = torch.optim.Adam(Node2vec.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "Node2vec.train()\n",
        "for epoch in range(epochs+150):\n",
        "    optimizer.zero_grad()\n",
        "    out = Node2vec(data)\n",
        "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "cCx5_Hl5CGCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Node2vec.eval()\n",
        "pred = Node2vec(data).argmax(dim=1)\n",
        "out = Node2vec(data)\n",
        "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
        "# print(out[data.test_mask])\n",
        "loss = loss_fn(out[data.test_mask], data.y[data.test_mask]).item()\n",
        "acc = int(correct) / int(data.test_mask.sum())\n",
        "print(f'loss:{loss:.4f}')\n",
        "Pred = pred[data.test_mask].cpu()\n",
        "Y = data.y[data.test_mask].cpu()\n",
        "C_Mat = metrics.confusion_matrix(Y, Pred)\n",
        "accuracy = metrics.accuracy_score(Y,Pred)\n",
        "f1 = metrics.f1_score(Y, Pred)\n",
        "recall = metrics.recall_score(Y, Pred)\n",
        "precision = metrics.precision_score(Y, Pred)\n",
        "print(C_Mat)\n",
        "print(f\"acc:{accuracy:.4f}, f1:{f1:.4f}, recall:{recall:.4f}, prec:{precision:.4f}\")\n"
      ],
      "metadata": {
        "id": "D44a9N5TCF6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QrQHIfk72sHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ku7F1HSs2sEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "w5fEBvVV2r-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save&Load Model"
      ],
      "metadata": {
        "id": "VNIQr9XuakxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 保存模型权重至当前文件夹\n",
        "torch.save(model.state_dict(), 'model_weights.pth')                             "
      ],
      "metadata": {
        "id": "tTO_IOiiarkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleNet(input_size, hidden_size, output_size)                         # 需要是同一个模型\n",
        "model.load_state_dict(torch.load('model_weights.pth'))\n",
        "model.eval()\n",
        "\n",
        "'''\n",
        "be sure to call model.eval() method before inferencing to set the dropout and batch normalization layers to evaluation mode. \n",
        "Failing to do this will yield inconsistent inference results.\n",
        "'''"
      ],
      "metadata": {
        "id": "nQejY8J8ay0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 直接保存/加载整个模型\n",
        "torch.save(model, 'model.pth')"
      ],
      "metadata": {
        "id": "u0CxBdcFdJkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('model.pth')"
      ],
      "metadata": {
        "id": "7HujEzt6d9k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PISkr-lNd_iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discoveries\n",
        "1. SimpleNet的训练情况似乎说明**模型并不能从数据中学到什么东西（特征可能没有提供什么信息）**。\n",
        "线性模型和MLP跑出来几乎没差别，说明问题不在模型上面，基本可以断定是数据（输入特征）的问题。"
      ],
      "metadata": {
        "id": "6GSfZNi978Mu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. GNN的训练好像比SimpleNet快很多，可能是train_loop写得太复杂了。"
      ],
      "metadata": {
        "id": "NzEJ90CXhX_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t = raw_data[\"patent_title\"].values[0] +\" \" +raw_data[\"patent_abstract\"].values[0]\n",
        "\n"
      ],
      "metadata": {
        "id": "AXbkrbORciD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i = tokenizer(t, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "Bt1SjH-kQD69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i, i.input_ids[0, :128]\n",
        "i.input_ids[0,:510].shape"
      ],
      "metadata": {
        "id": "maB48nxZQ6Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KrhGZ_qxRP5A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}