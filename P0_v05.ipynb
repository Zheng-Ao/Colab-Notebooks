{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P0_v05.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fKdAJ8l5F_yg",
        "X_w0ItLna28l",
        "eFTDdD4jgSP3",
        "wmFzeBC6qaHX",
        "DgQRy866G5JW",
        "VNIQr9XuakxG",
        "6GSfZNi978Mu"
      ],
      "mount_file_id": "1Sqr4KjJKKhLBjnKrl_PrfHPUOMsYKDXX",
      "authorship_tag": "ABX9TyPe2C+2qIzOmyw1737HQVmh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zheng-Ao/Colab-Notebooks/blob/main/P0_v05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# library install & cuda device"
      ],
      "metadata": {
        "id": "fKdAJ8l5F_yg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CR_-xEl5Emv2"
      },
      "outputs": [],
      "source": [
        "!python -c \"import torch; print(torch.__version__)\"\n",
        "# !python -c \"import torch; print(torch.version.cuda)\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "# !pip install torch-scatter -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
        "# !pip install torch-sparse -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
        "# !pip install torch-geometric"
      ],
      "metadata": {
        "id": "q_Zxt-SIFo9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "nCiAU3vwGSlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# utils.py"
      ],
      "metadata": {
        "id": "F_Rn1K8pE3Z0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Txt2Vec Matrix, PCA"
      ],
      "metadata": {
        "id": "X_w0ItLna28l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data_path = \"drive/MyDrive/P0/T10K.csv\"\n",
        "\n",
        "# 构建出txt_vecs，作为Dataset的第二个数据来源\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")      \n",
        "nlp_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
        "\n",
        "'''加载BERT这种大模型很耗时，因此在整个流程中应当让上面两行代码只执行一次。'''\n",
        "\n",
        "\n",
        "df = pd.read_csv(raw_data_path)\n",
        "\n",
        "for i in tqdm(range(10000)):\n",
        "    ttl = df[\"patent_title\"].values[i]\n",
        "    inputs = tokenizer(ttl, return_tensors=\"pt\").to(device)\n",
        "    outputs = nlp_model(**inputs)\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "    cls_vec = last_hidden_states[:,0,:].clone().detach()\n",
        "    if i == 0:\n",
        "        txt_vecs = cls_vec\n",
        "    else:\n",
        "        txt_vecs = torch.cat([txt_vecs, cls_vec],dim=0)"
      ],
      "metadata": {
        "id": "TjwGItGlE5Eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt_vecs.shape"
      ],
      "metadata": {
        "id": "WqiAYwXBdZoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Mat = txt_vecs.cpu().numpy()"
      ],
      "metadata": {
        "id": "wUIQrsOSb0X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Mat.shape"
      ],
      "metadata": {
        "id": "qCNKg7S-gnio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "E_SqxF1chNgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# txt_pca = PCA(n_components=0.9, svd_solver = 'full')\n",
        "txt_pca = PCA(n_components = 5)"
      ],
      "metadata": {
        "id": "3dy2-_CGoCqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# txt_pca.fit(Mat)\n",
        "# var = txt_pca.explained_variance_ratio_\n",
        "# len(var), var.sum()"
      ],
      "metadata": {
        "id": "CaoLuPwphNVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_Mat = txt_pca.fit_transform(Mat)"
      ],
      "metadata": {
        "id": "Wp1-4ZfciG91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## utils"
      ],
      "metadata": {
        "id": "eFTDdD4jgSP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, Subset, ConcatDataset, DataLoader\n",
        "from torch import nn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics\n",
        "\n",
        "import random\n",
        "\n",
        "from datetime import datetime\n",
        "# target transform, 0:neg, 1:pos\n",
        "def LabelCorePa(ref, isd):\n",
        "    now = datetime.strptime(\"2022-01-01\", \"%Y-%M-%d\").year\n",
        "    years = now - datetime.strptime(isd, \"%Y-%M-%d\").year\n",
        "    score = ref/years       \n",
        "    label = int((score>0.5))\n",
        "    return label\n",
        "\n",
        "# Dataset\n",
        "class PatDataset(Dataset):\n",
        "    def __init__(self, raw_data_path, txt_vecs, transform = None, target_transform = LabelCorePa):\n",
        "        self.raw_data = pd.read_csv(raw_data_path)\n",
        "        self.txt_vecs = txt_vecs\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.raw_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Y          \n",
        "        ref = self.raw_data.at[idx, \"patent_num_cited_by_us_patents\"]\n",
        "        isd = self.raw_data.at[idx, \"patent_date\"]\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(ref, isd)\n",
        "            label = torch.tensor(label,dtype=torch.long)\n",
        "\n",
        "        # X\n",
        "        # INDEXs\n",
        "        num_claims = self.raw_data.at[idx, \"patent_num_claims\"]\n",
        "        b_cits = self.raw_data.at[idx, \"patent_num_us_patent_citations\"]\n",
        "        inventors = self.raw_data.at[idx, \"inventors\"]\n",
        "        num_inventors = len(eval(inventors))\n",
        "        assignees = self.raw_data.at[idx, \"assignees\"]\n",
        "        if assignees == \"[{'assignee_sequence': None, 'assignee_key_id': None}]\":\n",
        "            num_assignees = 0\n",
        "        else:\n",
        "            num_assignees = len(eval(assignees))\n",
        "        IPCs = self.raw_data.at[idx, \"IPCs\"]\n",
        "        num_ipcs = len(eval(IPCs))\n",
        "        indexs = torch.tensor([num_claims, b_cits, num_inventors, num_assignees, num_ipcs], dtype=torch.float32).to(device)\n",
        "        # TXT\n",
        "        txt_vec = torch.tensor(self.txt_vecs[idx], dtype = torch.float32)\n",
        "        txt_vec = txt_vec.to(device)\n",
        "\n",
        "        patent = torch.cat([indexs, txt_vec])\n",
        "        \n",
        "        return patent, label\n",
        "\n",
        "# Models\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.BatchNorm1d(input_size),\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, output_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        # batch: 第几个batch；X: 包含batch_size个feature vec.\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        output = model(X).to(device)\n",
        "        loss = loss_fn(output, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 20 == 0:                                                     # train_size/batch_size = 100, 每20个batch输出一次结果，共输出5次。\n",
        "            loss, current = loss.item(), batch * len(X)                         \n",
        "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "# Test\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            output = model(X)\n",
        "            # test_loss += loss_fn(output, y).item()\n",
        "            # correct += (output.argmax(1) == y).type(torch.float).sum().item()\n",
        "            \n",
        "            pred = output.argmax(1).cpu()\n",
        "            y = y.cpu()\n",
        "            if batch == 0:\n",
        "                Pred = pred\n",
        "                Y = y\n",
        "            else:\n",
        "                Pred = torch.cat((Pred, pred), dim = 0)\n",
        "                Y = torch.cat((Y, y), dim=0)\n",
        "\n",
        "    C_Mat = metrics.confusion_matrix(Y, Pred)\n",
        "    accuracy = metrics.accuracy_score(Y,Pred)\n",
        "    f1 = metrics.f1_score(Y, Pred)\n",
        "    recall = metrics.recall_score(Y, Pred)\n",
        "    precision = metrics.precision_score(Y, Pred)\n",
        "    print(C_Mat)\n",
        "    print(f\"acc:{accuracy:.4f}, f1:{f1:.4f}, recall:{recall:.4f}, prec:{precision:.4f}\")\n",
        "\n",
        "    # test_loss /= num_batches\n",
        "    # correct /= size\n",
        "    # print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ],
      "metadata": {
        "id": "K8tSi8TZFQyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# config.py"
      ],
      "metadata": {
        "id": "wmFzeBC6qaHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HyperParams----Config.py\n",
        "hidden_size = 32\n",
        "output_size = 2\n",
        "\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 5e-4\n",
        "batch_size = 64                                                                 # test_size/batch_size = 25, 25 batches.\n",
        "epochs = 10\n",
        "\n",
        "num_train = 8000\n",
        "num_test = 2000"
      ],
      "metadata": {
        "id": "jKlYsn2aqdjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "DgQRy866G5JW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_Mat.shape"
      ],
      "metadata": {
        "id": "LKj3i6Uou6DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PIPLINE\n",
        "# DATA TO FIT A MODEL\n",
        "dataset = PatDataset(raw_data_path=raw_data_path, txt_vecs = new_Mat)\n",
        "training_indices = [i for i in range(num_train)]\n",
        "test_indices = [i for i in range(num_train,num_train+num_test)]\n",
        "training_data_all = Subset(dataset, training_indices)\n",
        "test_data = Subset(dataset, test_indices)\n",
        "\n",
        "# Make pos:neg in training set 1:1\n",
        "pos_indices = []\n",
        "neg_indices = []\n",
        "for i in range(num_train):\n",
        "    if training_data_all[i][1] == 1:\n",
        "        pos_indices.append(i)\n",
        "    else:\n",
        "        neg_indices.append(i)\n",
        "num_pos = len(pos_indices)\n",
        "num_neg = len(neg_indices)\n",
        "print(f\"{num_neg}:{num_pos} = {num_neg/num_pos}\")\n",
        "neg_indices_sample = random.choices(neg_indices, k=num_pos)\t \n",
        "training_data_pos = Subset(training_data_all, pos_indices)\n",
        "training_data_neg = Subset(training_data_all, neg_indices_sample)\n",
        "training_data = ConcatDataset([training_data_pos, training_data_neg])\n",
        "print(\"Training sample number:\",len(training_data))\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")"
      ],
      "metadata": {
        "id": "Cnjn-MyeOvYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FIT A MODEL\n",
        "# PyTorch的逻辑是先初始化（喂超参），再进行函数计算（喂输入）\n",
        "input_size = len(dataset[0][0])\n",
        "model = SimpleNet(input_size, hidden_size, output_size).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(train_dataloader, model, loss_fn)                                 # performance on training data\n",
        "    test_loop(test_dataloader, model, loss_fn)                                  # performance on test data\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "iZiFY_jDIhNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JT6lZqVrIsnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CtjHVuhNP3vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save&Load Model"
      ],
      "metadata": {
        "id": "VNIQr9XuakxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 保存模型权重至当前文件夹\n",
        "torch.save(model.state_dict(), 'model_weights.pth')                             "
      ],
      "metadata": {
        "id": "tTO_IOiiarkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleNet(input_size, hidden_size, output_size)                         # 需要是同一个模型\n",
        "model.load_state_dict(torch.load('model_weights.pth'))\n",
        "model.eval()\n",
        "\n",
        "'''\n",
        "be sure to call model.eval() method before inferencing to set the dropout and batch normalization layers to evaluation mode. \n",
        "Failing to do this will yield inconsistent inference results.\n",
        "'''"
      ],
      "metadata": {
        "id": "nQejY8J8ay0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 直接保存/加载整个模型\n",
        "torch.save(model, 'model.pth')"
      ],
      "metadata": {
        "id": "u0CxBdcFdJkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('model.pth')"
      ],
      "metadata": {
        "id": "7HujEzt6d9k-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PISkr-lNd_iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discoveries"
      ],
      "metadata": {
        "id": "6GSfZNi978Mu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "事实证明，只用ttl几乎相当于没有给模型提供有用信息，模型倾向于只预测其中一类。"
      ],
      "metadata": {
        "id": "wWQWhD2z8Bfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mKJ3OC1k7-al"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}